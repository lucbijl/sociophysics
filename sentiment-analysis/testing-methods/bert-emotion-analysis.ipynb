{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion analysis with BERT\n",
    "\n",
    "Using transformers with the distilled bert-base model on the emobank dataset, to perform emotion analysis based on the circumplex model.\n",
    "\n",
    "Written by Luc Bijl."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the emobank training and testing dataset from the datasets directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "      V     A     D                                               Text\n",
      "0   3.0   3.0   3.2        Remember what she said in my last letter? \"\n",
      "1   3.0   3.0   3.0                                                ..\"\n",
      "2  3.44   3.0  3.22  Goodwill helps people get off of public assist...\n",
      "3  3.55  3.27  3.46  Sherry learned through our Future Works class ...\n",
      "4   3.6   3.3   3.8  Coming to Goodwill was the first step toward m...\n",
      "\n",
      "Test Data:\n",
      "      V     A     D                                               Text\n",
      "0   2.8   3.1   2.8                          If I wasn't working here.\n",
      "1  3.27  3.36  3.36      I've got more than a job; I've got a career.\"\n",
      "2  2.86  3.29  3.29                           He has no time to waste.\n",
      "3   3.4   3.1   3.4  With the help of friends like you, Goodwill ha...\n",
      "4   3.0   2.6   3.1                                      Real results.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emobank_dataset = \"../../datasets/emobank/emobank.csv\"\n",
    "\n",
    "df_emobank = pd.read_csv(emobank_dataset, header=None, names=['Id', 'Split', 'V', 'A', 'D', 'Text'])\n",
    "\n",
    "df_raw_train = df_emobank[df_emobank['Split'] == 'train'].drop(columns=['Id', 'Split']).reset_index(drop=True)\n",
    "df_raw_test = df_emobank[df_emobank['Split'] == 'test'].drop(columns=['Id', 'Split']).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(df_raw_train.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_raw_test.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the training and testing dataset to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                                Text      V      A      D\n",
      "0        Remember what she said in my last letter? \"  0.200  0.200  0.280\n",
      "1                                                ..\"  0.200  0.200  0.200\n",
      "2  Goodwill helps people get off of public assist...  0.376  0.200  0.288\n",
      "3  Sherry learned through our Future Works class ...  0.420  0.308  0.384\n",
      "4  Coming to Goodwill was the first step toward m...  0.440  0.320  0.520\n",
      "\n",
      "Test Data:\n",
      "                                                Text      V      A      D\n",
      "0                          If I wasn't working here.  0.120  0.240  0.120\n",
      "1      I've got more than a job; I've got a career.\"  0.308  0.344  0.344\n",
      "2                           He has no time to waste.  0.144  0.316  0.316\n",
      "3  With the help of friends like you, Goodwill ha...  0.360  0.240  0.360\n",
      "4                                      Real results.  0.200  0.040  0.240\n"
     ]
    }
   ],
   "source": [
    "def normalize(n):\n",
    "    normal_n = (2*n - 5) / 5\n",
    "    return normal_n\n",
    "\n",
    "df_train = df_raw_train[['Text']].copy()\n",
    "df_test = df_raw_test[['Text']].copy()\n",
    "\n",
    "for i in ['V','A','D']:\n",
    "    df_train[i] = normalize(df_raw_train[i].astype(float))\n",
    "    df_test[i] = normalize(df_raw_test[i].astype(float))\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(df_train.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the most extreme sentences in the training set in either of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min V:\n",
      "Text    \"Fuck you\"\n",
      "V            -0.52\n",
      "A             0.68\n",
      "D             0.52\n",
      "Name: 930, dtype: object\n",
      "\n",
      "Max V:\n",
      "Text    lol Wonderful Simply Superb!\n",
      "V                               0.84\n",
      "A                               0.72\n",
      "D                               0.48\n",
      "Name: 7695, dtype: object\n",
      "\n",
      "\n",
      "Min A:\n",
      "Text    I was feeling calm and private that night.\n",
      "V                                             0.24\n",
      "A                                            -0.28\n",
      "D                                             0.24\n",
      "Name: 2859, dtype: object\n",
      "\n",
      "Max A:\n",
      "Text    \"My God, yes, yes, yes!\"\n",
      "V                           0.72\n",
      "A                           0.76\n",
      "D                           0.36\n",
      "Name: 6270, dtype: object\n",
      "\n",
      "\n",
      "Min D:\n",
      "Text    Hands closed on my neck and I felt my spine cr...\n",
      "V                                                   -0.24\n",
      "A                                                    0.52\n",
      "D                                                    -0.2\n",
      "Name: 3373, dtype: object\n",
      "\n",
      "Max D:\n",
      "Text    “NO”\n",
      "V      -0.32\n",
      "A       0.56\n",
      "D       0.68\n",
      "Name: 6481, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ['V','A','D']:\n",
    "    print(\"Min {}:\\n{}\".format(i, df_train.loc[df_train[i].argmin()]))\n",
    "    print()\n",
    "    print(\"Max {}:\\n{}\".format(i, df_train.loc[df_train[i].argmax()]))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the length of the training and testing dataset, to set a proper batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length training set: 8062\n",
      "Length testing set: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length training set: {len(df_train)}\\nLength testing set: {len(df_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if the GPU supports torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for BERT, this includes tokenization, encoding and creating dataloaders for both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luc/packages/anaconda/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizing and encoding the text data\n",
    "train_encodings = tokenizer(df_train['Text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(df_test['Text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    train_encodings['input_ids'], \n",
    "    train_encodings['attention_mask'], \n",
    "    torch.tensor(df_train[['V', 'A', 'D']].values, dtype=torch.float32)\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=29, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    test_encodings['input_ids'], \n",
    "    test_encodings['attention_mask'], \n",
    "    torch.tensor(df_test[['V', 'A', 'D']].values, dtype=torch.float32)\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model: distilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = L1Loss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the training loop, here BERT will be trained with the training dataset and validated with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.2365\n",
      "Epoch 1/30, Validation Loss: 0.1970\n",
      "\n",
      "Epoch 2/30, Training Loss: 0.1965\n",
      "Epoch 2/30, Validation Loss: 0.1871\n",
      "\n",
      "Epoch 3/30, Training Loss: 0.1843\n",
      "Epoch 3/30, Validation Loss: 0.1854\n",
      "\n",
      "Epoch 4/30, Training Loss: 0.1754\n",
      "Epoch 4/30, Validation Loss: 0.1853\n",
      "\n",
      "Epoch 5/30, Training Loss: 0.1654\n",
      "Epoch 5/30, Validation Loss: 0.1864\n",
      "\n",
      "Epoch 6/30, Training Loss: 0.1581\n",
      "Epoch 6/30, Validation Loss: 0.1851\n",
      "\n",
      "Epoch 7/30, Training Loss: 0.1513\n",
      "Epoch 7/30, Validation Loss: 0.1869\n",
      "\n",
      "Epoch 8/30, Training Loss: 0.1446\n",
      "Epoch 8/30, Validation Loss: 0.1876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = 'logs-4'\n",
    "writer = SummaryWriter(log_dir)\n",
    "global_step = 0\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "early_stop_patience = 2\n",
    "best_validation_loss = float('inf')\n",
    "no_improvement_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_v = 0\n",
    "    total_loss_a = 0\n",
    "    total_loss_d = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        global_step += 1\n",
    "        num_batches += 1\n",
    "        input_ids, attention_mask, target_scores = batch\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predicted_scores = output.logits\n",
    "\n",
    "        # Calculating the loss for each dimensions\n",
    "        loss_v = loss_fn(predicted_scores[:,0], target_scores[:,0])\n",
    "        loss_a = loss_fn(predicted_scores[:,1], target_scores[:,1])\n",
    "        loss_d = loss_fn(predicted_scores[:,2], target_scores[:,2])\n",
    "\n",
    "        # The main loss is defined as the sum of the individual losses\n",
    "        loss = loss_v + loss_a + loss_d\n",
    "\n",
    "        # The total loss per epoch\n",
    "        total_loss_v += loss_v.item()\n",
    "        total_loss_a += loss_a.item()\n",
    "        total_loss_d += loss_d.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Determining the average loss in the epoch\n",
    "        average_loss_v = total_loss_v / num_batches\n",
    "        average_loss_a = total_loss_a / num_batches\n",
    "        average_loss_d = total_loss_d / num_batches\n",
    "        average_loss = total_loss / num_batches\n",
    "\n",
    "        # Tensorboard logging\n",
    "        writer.add_scalar('Batch-loss-train-valence', average_loss_v, global_step)\n",
    "        writer.add_scalar('Batch-loss-train-arousal', average_loss_a, global_step)\n",
    "        writer.add_scalar('Batch-loss-train-dominance', average_loss_d, global_step)\n",
    "        writer.add_scalar('Batch-loss-train', average_loss, global_step)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Determining the average loss for the epoch\n",
    "    average_loss_v = total_loss_v / len(train_dataloader)\n",
    "    average_loss_a = total_loss_a / len(train_dataloader)\n",
    "    average_loss_d = total_loss_d / len(train_dataloader)\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Logging\n",
    "    writer.add_scalar('Epoch-loss-train-valence', average_loss_v, epoch + 1)\n",
    "    writer.add_scalar('Epoch-loss-train-arousal', average_loss_a, epoch + 1)\n",
    "    writer.add_scalar('Epoch-loss-train-dominance', average_loss_d, epoch + 1)\n",
    "    writer.add_scalar('Epoch-loss-train', average_loss, epoch + 1)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_v = 0\n",
    "    total_loss_a = 0\n",
    "    total_loss_d = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_mask, target_scores = batch\n",
    "\n",
    "            # Obtaining the scores\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predicted_scores = output.logits\n",
    "\n",
    "            # Calculating the loss for each dimensions\n",
    "            loss_v = loss_fn(predicted_scores[:,0], target_scores[:,0])\n",
    "            loss_a = loss_fn(predicted_scores[:,1], target_scores[:,1])\n",
    "            loss_d = loss_fn(predicted_scores[:,2], target_scores[:,2])\n",
    "            \n",
    "            # The main loss is defined as the sum of the individual losses\n",
    "            loss = loss_v + loss_a + loss_d\n",
    "\n",
    "            # The total loss per epoch\n",
    "            total_loss_v += loss_v.item()\n",
    "            total_loss_a += loss_a.item()\n",
    "            total_loss_d += loss_d.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Determining the average loss for the epoch\n",
    "    average_loss_v = total_loss_v / len(test_dataloader)\n",
    "    average_loss_a = total_loss_a / len(test_dataloader)\n",
    "    average_loss_d = total_loss_d / len(test_dataloader)\n",
    "    average_loss = total_loss / len(test_dataloader)\n",
    "\n",
    "    # Logging  \n",
    "    writer.add_scalar('Epoch-loss-validation-valence', average_loss_v, epoch + 1)\n",
    "    writer.add_scalar('Epoch-loss-validation-arousal', average_loss_a, epoch + 1)\n",
    "    writer.add_scalar('Epoch-loss-validation-dominance', average_loss_d, epoch + 1)\n",
    "    writer.add_scalar('Epoch-loss-validation', average_loss, epoch + 1)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {average_loss:.4f}\\n\")\n",
    "\n",
    "    # Saving the model\n",
    "    torch.save(model, f'bert-emobank-4/{epoch + 1}.pth')\n",
    "\n",
    "    # Early stopping check\n",
    "    if average_loss < best_validation_loss:\n",
    "        best_validation_loss = average_loss\n",
    "        no_improvement_counter = 0\n",
    "    else:\n",
    "        no_improvement_counter += 1\n",
    "\n",
    "    if no_improvement_counter >= early_stop_patience:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('bert-emobank-4/6.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model, with as output the MAE, MSE and R-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient (R) V: 0.7740\n",
      "Mean Squared Error (MSE) V: 0.0077\n",
      "Mean Absolute Error (MAE) V: 0.0638\n",
      "\n",
      "Pearson Correlation Coefficient (R) A: 0.5412\n",
      "Mean Squared Error (MSE) A: 0.0071\n",
      "Mean Absolute Error (MAE) A: 0.0655\n",
      "\n",
      "Pearson Correlation Coefficient (R) D: 0.4805\n",
      "Mean Squared Error (MSE) D: 0.0055\n",
      "Mean Absolute Error (MAE) D: 0.0558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "model.eval()\n",
    "list_predicted_scores = {'V': [], 'A': [], 'D': []}\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask, target_scores = batch\n",
    "\n",
    "        # Obtaining the scores\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predicted_scores = output.logits\n",
    "\n",
    "        # Writing the scores to the list\n",
    "        list_predicted_scores['V'].extend(predicted_scores[:, 0].tolist())\n",
    "        list_predicted_scores['A'].extend(predicted_scores[:, 1].tolist())\n",
    "        list_predicted_scores['D'].extend(predicted_scores[:, 2].tolist())\n",
    "\n",
    "# Inserting the scores in df_test\n",
    "for i,j in zip(['V', 'A', 'D'],['V-p', 'A-p', 'D-p']):\n",
    "    df_test[j] = list_predicted_scores[i]\n",
    "\n",
    "# Computing the R, MSE and MAE values.\n",
    "for i,j in zip(['V', 'A', 'D'],['V-p', 'A-p', 'D-p']):\n",
    "\n",
    "    correlation, _ = pearsonr(df_test[i], df_test[j])\n",
    "\n",
    "    print(f\"Pearson Correlation Coefficient (R) {i}: {correlation:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE) {i}: {mean_squared_error(df_test[i], df_test[j]):.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE) {i}: {mean_absolute_error(df_test[i], df_test[j]):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "      <th>V-p</th>\n",
       "      <th>A</th>\n",
       "      <th>A-p</th>\n",
       "      <th>D</th>\n",
       "      <th>D-p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.191236</td>\n",
       "      <td>0.196919</td>\n",
       "      <td>0.213576</td>\n",
       "      <td>0.210068</td>\n",
       "      <td>0.225812</td>\n",
       "      <td>0.230005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.138137</td>\n",
       "      <td>0.107967</td>\n",
       "      <td>0.098377</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.084154</td>\n",
       "      <td>0.042859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.372000</td>\n",
       "      <td>-0.195293</td>\n",
       "      <td>-0.160000</td>\n",
       "      <td>0.078601</td>\n",
       "      <td>-0.288000</td>\n",
       "      <td>0.064148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.149089</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.167158</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.207812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.210195</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.195651</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.230044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.251653</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.239414</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.254736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.695105</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.636425</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.416937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V          V-p            A          A-p            D  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.191236     0.196919     0.213576     0.210068     0.225812   \n",
       "std       0.138137     0.107967     0.098377     0.068097     0.084154   \n",
       "min      -0.372000    -0.195293    -0.160000     0.078601    -0.288000   \n",
       "25%       0.120000     0.149089     0.156000     0.167158     0.200000   \n",
       "50%       0.200000     0.210195     0.200000     0.195651     0.240000   \n",
       "75%       0.256000     0.251653     0.280000     0.239414     0.280000   \n",
       "max       0.720000     0.695105     0.680000     0.636425     0.600000   \n",
       "\n",
       "               D-p  \n",
       "count  1000.000000  \n",
       "mean      0.230005  \n",
       "std       0.042859  \n",
       "min       0.064148  \n",
       "25%       0.207812  \n",
       "50%       0.230044  \n",
       "75%       0.254736  \n",
       "max       0.416937  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['V', 'V-p', 'A', 'A-p', 'D', 'D-p']].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the most extreme sentences in the test set in either of the six dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min V:\n",
      "Text    Bangladesh ferry sinks, 15 dead\n",
      "V                                -0.372\n",
      "A                                   0.2\n",
      "D                                 0.028\n",
      "V-p                           -0.119922\n",
      "A-p                            0.279011\n",
      "D-p                            0.114692\n",
      "Name: 526, dtype: object\n",
      "\n",
      "Max V:\n",
      "Text    “That’s amazing!”\n",
      "V                    0.72\n",
      "A                    0.68\n",
      "D                     0.4\n",
      "V-p              0.695105\n",
      "A-p              0.597201\n",
      "D-p              0.319575\n",
      "Name: 382, dtype: object\n",
      "\n",
      "\n",
      "Min V-p:\n",
      "Text    Bomb kills 18 on military bus in Iran\n",
      "V                                       -0.08\n",
      "A                                        0.28\n",
      "D                                        0.08\n",
      "V-p                                 -0.195293\n",
      "A-p                                  0.364857\n",
      "D-p                                  0.122096\n",
      "Name: 461, dtype: object\n",
      "\n",
      "Max V-p:\n",
      "Text    “That’s amazing!”\n",
      "V                    0.72\n",
      "A                    0.68\n",
      "D                     0.4\n",
      "V-p              0.695105\n",
      "A-p              0.597201\n",
      "D-p              0.319575\n",
      "Name: 382, dtype: object\n",
      "\n",
      "\n",
      "Min A:\n",
      "Text    But in fact, once news of the handover vanishe...\n",
      "V                                                    0.16\n",
      "A                                                   -0.16\n",
      "D                                                    0.32\n",
      "V-p                                              0.200088\n",
      "A-p                                              0.160717\n",
      "D-p                                              0.212748\n",
      "Name: 247, dtype: object\n",
      "\n",
      "Max A:\n",
      "Text    “That’s amazing!”\n",
      "V                    0.72\n",
      "A                    0.68\n",
      "D                     0.4\n",
      "V-p              0.695105\n",
      "A-p              0.597201\n",
      "D-p              0.319575\n",
      "Name: 382, dtype: object\n",
      "\n",
      "\n",
      "Min A-p:\n",
      "Text    Name Address City, ST Zip\n",
      "V                             0.2\n",
      "A                            0.12\n",
      "D                             0.2\n",
      "V-p                      0.209377\n",
      "A-p                      0.078601\n",
      "D-p                      0.207376\n",
      "Name: 71, dtype: object\n",
      "\n",
      "Max A-p:\n",
      "Text    She did!\n",
      "V           0.32\n",
      "A           0.36\n",
      "D           0.36\n",
      "V-p     0.310692\n",
      "A-p     0.636425\n",
      "D-p     0.352002\n",
      "Name: 767, dtype: object\n",
      "\n",
      "\n",
      "Min D:\n",
      "Text    I shivered as I walked past the pale man’s bla...\n",
      "V                                                    -0.2\n",
      "A                                                     0.2\n",
      "D                                                  -0.288\n",
      "V-p                                              0.032447\n",
      "A-p                                              0.244601\n",
      "D-p                                              0.100299\n",
      "Name: 374, dtype: object\n",
      "\n",
      "Max D:\n",
      "Text    I'll make you fight.\n",
      "V                      -0.04\n",
      "A                       0.52\n",
      "D                        0.6\n",
      "V-p                 0.271099\n",
      "A-p                 0.314649\n",
      "D-p                 0.377356\n",
      "Name: 721, dtype: object\n",
      "\n",
      "\n",
      "Min D-p:\n",
      "Text    Swiss plane crashes at Moscow airport\n",
      "V                                       -0.12\n",
      "A                                         0.2\n",
      "D                                        0.24\n",
      "V-p                                 -0.096642\n",
      "A-p                                  0.294514\n",
      "D-p                                  0.064148\n",
      "Name: 457, dtype: object\n",
      "\n",
      "Max D-p:\n",
      "Text    Start your own!\n",
      "V                  0.16\n",
      "A                  0.44\n",
      "D                  0.36\n",
      "V-p            0.350196\n",
      "A-p            0.601057\n",
      "D-p            0.416937\n",
      "Name: 583, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ['V', 'V-p', 'A', 'A-p', 'D', 'D-p']:\n",
    "    print(\"Min {}:\\n{}\".format(i, df_test.loc[df_test[i].argmin()]))\n",
    "    print()\n",
    "    print(\"Max {}:\\n{}\".format(i, df_test.loc[df_test[i].argmax()]))\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c75c0fdd1a718867cdcb84b32adcfdbeaad00b3a4e00a59385211aeed084d4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
